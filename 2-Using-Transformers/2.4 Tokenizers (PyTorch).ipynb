{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OT5-61tCr_-E"
   },
   "source": [
    "# Tokenizers (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWZlzcUUr_-F"
   },
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T08:10:12.075074Z",
     "start_time": "2023-07-04T08:10:12.067622Z"
    },
    "id": "GFcTR0FXr_-G"
   },
   "outputs": [],
   "source": [
    "# !pip install datasets evaluate transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><b>Tokenizers</b> are one of the core components of the NLP pipeline. They serve one purpose: to <b>translate text into data that can be processed by the model</b>. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we’ll explore exactly what happens in the tokenization pipeline.</span>\n",
    "\n",
    "In NLP tasks, the data that is generally processed is raw text. Here’s an example of such text:\n",
    "\n",
    "```\n",
    "Jim Henson was a puppeteer\n",
    "```\n",
    "\n",
    "However, models can only process numbers, so we need to find a way to convert the raw text to numbers. That’s what the tokenizers do, and there are a lot of ways to go about this. <span style=\"color:blue\">The goal is to find the most meaningful representation — that is, the one that makes the most sense to the model — and, if possible, the smallest representation.</span>\n",
    "\n",
    "<img src=\"images/tokenizer_1.png\" style=\"width:1000px;\" title=\"Tokenizer\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at <b>some examples of tokenization algorithms</b>, and try to answer some of the questions you may have about tokenization.\n",
    "\n",
    "- <b>Word-based</b>\n",
    "- <b>Character-based</b>\n",
    "- <b>Sub-word based</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first type of tokenizer that comes to mind is <i>word-based</i>. It’s generally very easy to set up and use with only a few rules, and it often yields decent results. For example, in the image below, the goal is to <b>split the raw text into words</b> and find a numerical representation for each of them:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg\" style=\"width:800px;\" title=\"Word based tokenization\">\n",
    "<hr>\n",
    "<img src=\"images/word-based-tokenization-1.png\" style=\"width:800px;\" title=\"Word based tokenization\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways to split the text. For example, we could use whitespace to tokenize the text into words by applying Python’s `split()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:46:59.554728Z",
     "start_time": "2023-07-04T12:46:59.547090Z"
    },
    "id": "gg9iCSI2r_-H",
    "outputId": "9b2a2b9b-031b-4136-abbc-84b02146a689"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jim', 'Henson', 'was', 'a', 'puppeteer']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also variations of word tokenizers that have extra rules for punctuation. With this kind of tokenizer, we can end up with some pretty large “vocabularies,” where a <span style=\"color:blue\"><b>vocabulary is defined by the total number of independent tokens that we have in our corpus</b></span>.\n",
    "\n",
    "Each word gets assigned an ID, starting from 0 and going up to the size of the vocabulary. The model uses these IDs to identify each word.\n",
    "\n",
    "<span style=\"color:red\">If we want to completely cover a language with a word-based tokenizer, we’ll need to have an identifier for each word in the language, which will generate a <b>huge amount of tokens (large vocabularies result in heavy models)</b>. For example, there are over 500,000 words in the English language, so to build a map from each word to an input ID we’d need to keep track of that many IDs. Furthermore, words like “dog” are represented differently from words like “dogs”, and the model will initially have no way of knowing that “dog” and “dogs” are similar: it will <b>identify the two words as unrelated</b>. The same applies to other similar words, like “run” and “running”, which the model will not see as being similar initially.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/word-based-tokenization-issues-1.png\" style=\"width:500px;\" title=\"Word based tokenization issues\">\n",
    "<img src=\"images/word-based-tokenization-issues-2.png\" style=\"width:400px;\" title=\"Word based tokenization issues\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Finally, we need a <b>custom token</b> to represent words that are not in our vocabulary. This is known as the <b>“unknown” token</b>, often represented as <b>”[UNK]”</b> or <b>””</b>. </span><span style=\"color:red\">It’s generally a bad sign if you see that the tokenizer is producing a lot of these tokens, as it wasn’t able to retrieve a sensible representation of a word and you’re losing information along the way. <span style=\"color:green\"><b>The goal when crafting the vocabulary is to do it in such a way that the tokenizer tokenizes as few words as possible into the unknown token.</b></span>\n",
    "\n",
    "<span style=\"color:green\">One naive way to overcome the issue and build smaller vocabularies is to limit the amount of words we add to the vocabulary. For e.g., pick the top 10000 most frequently occuring words in the corpus and build the vocabulary.</span> Any other word out of the vocabulary could be marked as `\"UNKNOWN\"`.\n",
    "\n",
    "<img src=\"images/word-based-tokenizer-limit-tokens-issue.png\" style=\"width:800px;\" title=\"Word based tokenization issues\">\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">One way to reduce the amount of unknown tokens is to go one level deeper, using a <i>character-based</i> tokenizer.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character-based tokenizers split the text into characters, rather than words. <span style=\"color:green\">This has two primary benefits</span>:\n",
    "\n",
    "- <span style=\"color:green\">The vocabulary is much smaller.<span>\n",
    "- <span style=\"color:blue\">There are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters.</span>\n",
    "\n",
    "<span style=\"color:red\">But here too some questions arise concerning spaces and punctuation:</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization.svg\" style=\"width:800px;\" title=\"Character based tokenization\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">This approach isn’t perfect either. Since the representation is now based on characters rather than words, one could argue that, intuitively, <b>it’s less meaningful</b>: each character doesn’t mean a lot on its own</b>, whereas that is the case with words. </span> <span style=\"color:green\">However, this again differs according to the language; in Chinese, for example, each character carries more information than a character in a Latin language.</span>\n",
    "\n",
    "<span style=\"color:red\">Another thing to consider is that we’ll end up with a <b>very large amount of tokens to be processed by our model</b>: whereas a word would only be a single token with a word-based tokenizer, it can easily turn into 10 or more tokens when converted into characters</span>.\n",
    "\n",
    "<span style=\"color:green\">To get the best of both worlds, we can use a third technique that combines the two approaches: <b><i>subword tokenization</i>.</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subword tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Subword tokenization algorithms rely on the principle that <b>frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords</b>.</span>\n",
    "\n",
    "For instance, “annoyingly” might be considered a rare word and could be decomposed into “annoying” and “ly”. These are both likely to appear more frequently as standalone subwords, while at the same time the meaning of “annoyingly” is kept by the composite meaning of “annoying” and “ly”.\n",
    "\n",
    "Here is an example showing how a subword tokenization algorithm would tokenize the sequence “Let’s do tokenization!“:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword.svg\" style=\"width:800px;\" title=\"Subword based tokenization\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">These subwords end up providing a <b>lot of semantic meaning</b>: for instance, in the example above “tokenization” was split into “token” and “ization”, two tokens that have a semantic meaning while being space-efficient (only two tokens are needed to represent a long word). This allows us to have <b>relatively good coverage with small vocabularies, and close to no unknown tokens</b>.</span>\n",
    "\n",
    "This approach is especially useful in agglutinative languages such as Turkish, where you can form (almost) arbitrarily long complex words by stringing together subwords.\n",
    "\n",
    "### And more!\n",
    "Unsurprisingly, there are many more techniques out there. To name a few:\n",
    "\n",
    "- Byte-level BPE, as used in GPT-2\n",
    "- WordPiece, as used in BERT\n",
    "- SentencePiece or Unigram, as used in several multilingual models\n",
    "\n",
    "You should now have sufficient knowledge of how tokenizers work to get started with the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and saving tokenizers is as simple as it is with models. Actually, it’s based on the same two methods: `from_pretrained()` and `save_pretrained()`. <span style=\"color:green\">These methods will load or save the algorithm used by the tokenizer (a bit like the <i>architecture</i> of the model) as well as its vocabulary (a bit like the <i>weights</i> of the model).</span>\n",
    "\n",
    "Loading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading the model, except we use the `BertTokenizer` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:47:02.388241Z",
     "start_time": "2023-07-04T12:47:02.069066Z"
    },
    "id": "QPx-fAXDr_-H"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to `AutoModel`, the `AutoTokenizer` class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:47:03.318733Z",
     "start_time": "2023-07-04T12:47:03.012998Z"
    },
    "id": "mD-egaPyr_-H"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the tokenizer as shown in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:46:56.849778Z",
     "start_time": "2023-07-04T12:46:56.841855Z"
    },
    "id": "k-NqW-kJr_-I",
    "outputId": "11c37f70-a5da-461b-9f0d-a6f656aa8b87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Using a Transformer network is simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving a tokenizer is identical to saving a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:47:49.622764Z",
     "start_time": "2023-07-04T12:47:49.578731Z"
    },
    "id": "hgufBpzxr_-I"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('directory_on_my_computer/tokenizer_config.json',\n",
       " 'directory_on_my_computer/special_tokens_map.json',\n",
       " 'directory_on_my_computer/vocab.txt',\n",
       " 'directory_on_my_computer/added_tokens.json',\n",
       " 'directory_on_my_computer/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"directory_on_my_computer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:49:03.519766Z",
     "start_time": "2023-07-04T12:49:03.375225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "total 853856\r\n",
      "drwxr-xr-x   8 prasanth.thangavel  staff   256B Jul  4 20:47 \u001b[34m.\u001b[m\u001b[m/\r\n",
      "drwxr-xr-x  11 prasanth.thangavel  staff   352B Jul  4 20:48 \u001b[34m..\u001b[m\u001b[m/\r\n",
      "-rw-r--r--   1 prasanth.thangavel  staff   656B Jul  4 16:07 config.json\r\n",
      "-rw-r--r--   1 prasanth.thangavel  staff   413M Jul  4 16:07 pytorch_model.bin\r\n",
      "-rw-r--r--   1 prasanth.thangavel  staff   125B Jul  4 20:47 special_tokens_map.json\r\n",
      "-rw-r--r--   1 prasanth.thangavel  staff   653K Jul  4 20:47 tokenizer.json\r\n",
      "-rw-r--r--   1 prasanth.thangavel  staff   315B Jul  4 20:47 tokenizer_config.json\r\n",
      "-rw-r--r--   1 prasanth.thangavel  staff   208K Jul  4 20:47 vocab.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls -a -l -h directory_on_my_computer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll talk more about `token_type_ids` in Chapter 3, and we’ll explain the `attention_mask` key a little later. First, let’s see how the input_ids are generated. To do this, we’ll need to look at the intermediate methods of the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Translating text to numbers is known as <i>encoding</i>. Encoding is done in a two-step process: the tokenization, followed by the conversion to input IDs.</span>\n",
    "\n",
    "As we’ve seen, <b>the first step is to split the text into word (or parts of words, punctuation symbols, etc.), usually called <i>tokens</i></b>. There are multiple rules that can govern that process, which is why we need to instantiate the tokenizer using the name of the model, to <b>make sure we use the same rules that were used when the model was pretrained</b>.\n",
    "\n",
    "<b>The second step is to convert those tokens into numbers</b>, so we can build a tensor out of them and feed them to the model. To do this, the tokenizer has a <i>vocabulary</i>, which is the part we download when we instantiate it with the `from_pretrained()` method. Again, we need to <b>use the same vocabulary used when the model was pretrained</b>.\n",
    "\n",
    "To get a better understanding of the two steps, we’ll explore them separately. Note that we will use some methods that perform parts of the tokenization pipeline separately to show you the intermediate results of those steps, but in practice, you should call the tokenizer directly on your inputs (as shown in the section 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:33:11.056322Z",
     "start_time": "2023-07-04T12:33:11.047578Z"
    }
   },
   "source": [
    "The tokenization process is done by the `tokenize()` method of the tokenizer. The output of this method is a list of strings, or tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T12:59:42.072693Z",
     "start_time": "2023-07-04T12:59:41.513965Z"
    },
    "id": "LPYk7J0Pr_-I",
    "outputId": "afc7bebf-3445-4801-ea74-a600f937f35d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">This tokenizer is a <b>subword tokenizer</b>: it splits the words until it obtains tokens that can be represented by its vocabulary. That’s the case here with `Transformer`, which is split into two tokens: `Trans` and `##former`.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From tokens to input IDs\n",
    "\n",
    "The conversion to input IDs is handled by the `convert_tokens_to_ids()` tokenizer method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJAJ6SO6r_-I",
    "outputId": "eeaf6d1a-d954-4457-c248-a344d4c028c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7993, 170, 11303, 1200, 2443, 1110, 3014]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">These outputs (tokens), once converted to the appropriate framework tensor, can then be used as inputs to a model</span> as seen earlier in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><i>Decoding</i> is going the other way around: <b>from vocabulary indices, we want to get a string</b></span>. This can be done with the `decode()` method as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVLwnd0Vr_-J",
    "outputId": "8952b079-fa38-4ad5-f08f-33c403bec0b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Using a Transformer network is simple'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">Note that the decode method not only converts the indices back to tokens, but <b>also groups together the tokens that were part of the same words to produce a readable sentence</b>. This behavior will be extremely useful when we use models that predict new text (either <b>text generated from a prompt<b>, or for <b>sequence-to-sequence problems</b> like <b>translation</b> or <b>summarization</b>).</span>\n",
    "\n",
    "By now you should understand the atomic operations a tokenizer can handle: tokenization, conversion to IDs, and converting IDs back to a string. However, we’ve just scraped the tip of the iceberg. In the following section, we’ll take our approach to its limits and take a look at how to overcome them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Tokenizers (PyTorch)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "fastai_related",
   "language": "python",
   "name": "fastai_related"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
