{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://huggingface.co/learn/nlp-course/chapter2/6?fw=pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sC6AhyKvsHWT"
   },
   "source": [
    "# Putting it all together (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0B9Ck9rsHWV"
   },
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T00:04:06.598569Z",
     "start_time": "2023-07-15T00:04:06.594829Z"
    },
    "id": "leArGeznsHWV"
   },
   "outputs": [],
   "source": [
    "# !pip install datasets evaluate transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last few sections, weâ€™ve been trying our best to do most of the work by hand. Weâ€™ve <span style=\"color:blue\">explored how tokenizers work and looked at tokenization, conversion to input IDs, padding, truncation, and attention masks.</span>\n",
    "\n",
    "However, as we saw in section 2, the ðŸ¤— Transformers API can handle all of this for us with a high-level function that weâ€™ll dive into here. When you call your tokenizer directly on the sentence, you get back inputs that are ready to pass through your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T03:51:04.538522Z",
     "start_time": "2023-07-15T03:51:03.292104Z"
    },
    "id": "XbYjrnjBsHWW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)\n",
    "print (model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the `model_inputs` variable contains everything thatâ€™s necessary for a model to operate well. For <i>DistilBERT</i>, that includes the <i>input IDs</i> as well as the <i>attention mask</i>. Other models that accept additional inputs will also have those output by the tokenizer object.\n",
    "\n",
    "As weâ€™ll see in some examples below, this method is very powerful. First, it can tokenize a single sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T03:51:07.379757Z",
     "start_time": "2023-07-15T03:51:07.376060Z"
    },
    "id": "AZC9GPxYsHWW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)\n",
    "print (model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also handles multiple sequences at a time, with no change in the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T03:51:09.307828Z",
     "start_time": "2023-07-15T03:51:09.305265Z"
    },
    "id": "B17u0PJQsHWW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "model_inputs = tokenizer(sequences)\n",
    "print (model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can pad according to several objectives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T03:51:10.661971Z",
     "start_time": "2023-07-15T03:51:10.657807Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences[0]), len(sequences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T03:51:10.861539Z",
     "start_time": "2023-07-15T03:51:10.858849Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_inputs_len(input_ids):\n",
    "    if type(input_ids[0]) == list: # If list of list\n",
    "        return [len(input_id) for input_id in input_ids]\n",
    "    elif type(input_ids) == list: # If list\n",
    "        return len(input_ids)\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T03:51:12.241204Z",
     "start_time": "2023-07-15T03:51:12.235344Z"
    },
    "id": "9niyJ01ZsHWX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 6]\n",
      "[16, 16]\n",
      "[512, 512]\n",
      "[16, 8]\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer(sequences)\n",
    "print (get_model_inputs_len(model_inputs['input_ids']))\n",
    "\n",
    "# Will pad the sequences up to the maximum sequence length\n",
    "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
    "print (get_model_inputs_len(model_inputs['input_ids']))\n",
    "\n",
    "# Will pad the sequences up to the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
    "print (get_model_inputs_len(model_inputs['input_ids']))\n",
    "\n",
    "# Will pad the sequences up to the specified max length\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)\n",
    "print (get_model_inputs_len(model_inputs['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also truncate sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T03:51:13.387395Z",
     "start_time": "2023-07-15T03:51:13.383819Z"
    },
    "id": "ZLYDFnzpsHWX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 6]\n",
      "[16, 6]\n",
      "[8, 6]\n"
     ]
    }
   ],
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "model_inputs = tokenizer(sequences)\n",
    "print (get_model_inputs_len(model_inputs['input_ids']))\n",
    "\n",
    "# Will truncate the sequences that are longer than the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, truncation=True)\n",
    "print (get_model_inputs_len(model_inputs['input_ids']))\n",
    "\n",
    "# Will truncate the sequences that are longer than the specified max length\n",
    "model_inputs = tokenizer(sequences, max_length=8, truncation=True)\n",
    "print (get_model_inputs_len(model_inputs['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tokenizer` object can handle the conversion to specific framework tensors, which can then be directly sent to the model. For example, in the following code sample we are prompting the tokenizer to return tensors from the different frameworks â€” `\"pt\"` returns PyTorch tensors, `\"tf\"` returns TensorFlow tensors, and `\"np\"` returns NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T03:51:16.107139Z",
     "start_time": "2023-07-15T03:51:14.734243Z"
    },
    "id": "4koCakYMsHWX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# Returns PyTorch tensors\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
    "print (type(model_inputs['input_ids']))\n",
    "\n",
    "# Returns TensorFlow tensors\n",
    "# model_inputs = tokenizer(sequences, padding=True, return_tensors=\"tf\")\n",
    "# print (type(model_inputs['input_ids']))\n",
    "\n",
    "# Returns NumPy arrays\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")\n",
    "print (type(model_inputs['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special tokens\n",
    "\n",
    "If we take a look at the input IDs returned by the tokenizer, we will see they are a tiny bit different from what we had earlier when we do it manually step-by-step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T03:51:16.446498Z",
     "start_time": "2023-07-15T03:51:16.443040Z"
    },
    "id": "Z5jSR1HisHWX",
    "outputId": "8d4a40a2-4e1b-4220-de4b-f1391cd08738"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102] \n",
      "\n",
      "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n",
      "[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)\n",
    "print(model_inputs[\"input_ids\"], \"\\n\")\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "final_ids = tokenizer.prepare_for_model(ids)\n",
    "print (final_ids['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">One token ID was added at the beginning, and one at the end. Letâ€™s decode the two sequences of IDs above to see what this is about:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T03:51:18.122682Z",
     "start_time": "2023-07-15T03:51:18.115677Z"
    },
    "id": "OTYzTmzGsHWY",
    "outputId": "f3c439cb-b42f-4526-dee6-76cffd209e85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] i've been waiting for a huggingface course my whole life. [SEP]\n",
      "i've been waiting for a huggingface course my whole life.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(model_inputs[\"input_ids\"]))\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">The tokenizer added the special word `[CLS]` at the beginning and the special word `[SEP]` at the end. This is because the model was pretrained with those, so to get the same results for inference we need to add them as well. Note that some models donâ€™t add special words, or add different ones; models may also add these special words only at the beginning, or only at the end. In any case, the tokenizer knows which ones are expected and will deal with this for you.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up: From tokenizer to model\n",
    "\n",
    "Now that weâ€™ve seen all the individual steps the `tokenizer` object uses when applied on texts, letâ€™s see one final time <span style=\"color:green\">how it can handle multiple sequences (padding!), very long sequences (truncation!), and multiple types of tensors</span> with its main API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T03:52:54.717076Z",
     "start_time": "2023-07-15T03:52:52.073610Z"
    },
    "id": "u0NO-vIksHWY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T03:52:54.724741Z",
     "start_time": "2023-07-15T03:52:54.718681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T03:52:54.728172Z",
     "start_time": "2023-07-15T03:52:54.725843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "print (tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T03:52:54.738031Z",
     "start_time": "2023-07-15T03:52:54.730025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5607,  1.6123],\n",
      "        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print (output['logits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T03:52:54.742179Z",
     "start_time": "2023-07-15T03:52:54.739329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NEGATIVE': 0, 'POSITIVE': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic usage completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job following the course up to here! To recap, in this chapter you:\n",
    "\n",
    "- Learned the basic building blocks of a Transformer model.\n",
    "- Learned what makes up a tokenization pipeline.\n",
    "- Saw how to use a Transformer model in practice.\n",
    "- Learned how to leverage a tokenizer to convert text to tensors that are understandable by the model.\n",
    "- Set up a tokenizer and a model together to get from text to predictions.\n",
    "- Learned the limitations of input IDs, and learned about attention masks.\n",
    "- Played around with versatile and configurable tokenizer methods.\n",
    "\n",
    "From now on, you should be able to freely navigate the ðŸ¤— Transformers docs: the vocabulary will sound familiar, and youâ€™ve already seen the methods that youâ€™ll use the majority of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Putting it all together (PyTorch)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "fastai_related",
   "language": "python",
   "name": "fastai_related"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
