{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b46a6546",
   "metadata": {},
   "source": [
    "Source: https://huggingface.co/learn/nlp-course/chapter1/8?fw=pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c7ff4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T02:15:06.689921Z",
     "start_time": "2023-07-04T02:15:06.680030Z"
    }
   },
   "source": [
    "# Bias and limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f9799a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1758325e",
   "metadata": {},
   "source": [
    "If your intent is to use a pretrained model or a fine-tuned version in production, please be aware that, while these models are powerful tools, they come with limitations. The biggest of these is that, to enable pretraining on large amounts of data, <span style=\"color:blue\">researchers often scrape all the content they can find, <span style=\"color:green\">taking the best </span><span style=\"color:red\">as well as the worst</span> of what is available on the internet</span>.\n",
    "\n",
    "To give a quick illustration, let‚Äôs go back the example of a `fill-mask` pipeline with the BERT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a69e3d8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T02:16:55.994961Z",
     "start_time": "2023-07-04T02:16:54.403882Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n",
      "['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "\n",
    "result = unmasker(\"This man works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"This woman works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3a6423",
   "metadata": {},
   "source": [
    "When asked to fill in the missing word in these two sentences, <span style=\"color:red\">the model gives only one gender-free answer (waiter/waitress). The others are work occupations usually associated with one specific gender ‚Äî and yes, prostitute ended up in the top 5 possibilities the model associates with ‚Äúwoman‚Äù and ‚Äúwork‚Äù</span>. This happens even though BERT is one of the rare Transformer models not built by scraping data from all over the internet, but rather using apparently neutral data (it‚Äôs trained on the English Wikipedia and BookCorpus datasets).\n",
    "\n",
    "When you use these tools, you therefore need to keep in the back of your mind that the <span style=\"color:red\">original model you are using could very easily generate sexist, racist, or homophobic content. <b>Fine-tuning the model on your data won‚Äôt make this intrinsic bias disappear</b></span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdf0dab",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795214e7",
   "metadata": {},
   "source": [
    "In this chapter, you saw how to approach different NLP tasks using the 1high-level pipeline()1 function from ü§ó Transformers. You also saw how to search for and use models in the Hub, as well as how to use the Inference API to test the models directly in your browser.\n",
    "\n",
    "We discussed how Transformer models work at a high level, and talked about the importance of transfer learning and fine-tuning. <span style=\"color:blue\">A key aspect is that you can use the full architecture or only the encoder or decoder, depending on what kind of task you aim to solve</span>. The following table summarizes this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c738f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T02:20:38.670809Z",
     "start_time": "2023-07-04T02:20:38.660111Z"
    }
   },
   "source": [
    "| Model |\tExamples |\tTasks |\n",
    "| :- | :- | :- |\n",
    "| Encoder\t| ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa\t| Sentence classification, named entity recognition, extractive question answering | \n",
    "| Decoder\t| CTRL, GPT, GPT-2, Transformer XL\t| Text generation |\n",
    "| Encoder-decoder\t| BART, T5, Marian, mBART\t| Summarization, translation, generative question answering |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a55efaf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T02:25:39.964165Z",
     "start_time": "2023-07-04T02:25:34.481844Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'This is a course about the Transformers library', 'labels': ['educational', 'not educational'], 'scores': [0.9890398979187012, 0.010960041545331478]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "result = classifier(\"This is a course about the Transformers library\", \n",
    "                    candidate_labels=[\"educational\", \"not educational\"])\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c4203e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai_related",
   "language": "python",
   "name": "fastai_related"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
