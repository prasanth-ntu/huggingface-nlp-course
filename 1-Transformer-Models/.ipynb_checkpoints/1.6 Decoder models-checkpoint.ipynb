{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04c80644",
   "metadata": {},
   "source": [
    "Source: https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d37607",
   "metadata": {},
   "source": [
    "# Decoder models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64310ad",
   "metadata": {},
   "source": [
    "Decoder models use only the decoder of a Transformer model. <span style=\"color:blue\">At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called <b><i>auto-regressive models</i></b></span>.\n",
    "\n",
    "<span style=\"color:blue\">The <b>pretraining</b> of decoder models usually revolves around <b>predicting the next word in the sentence.</b></span>\n",
    "\n",
    "These models are <span style=\"color:blue\">best suited for tasks involving <b>text generation</b></span>.\n",
    "\n",
    "Representatives of this family of models include:\n",
    "- [CTRL](https://huggingface.co/transformers/model_doc/ctrl.html)\n",
    "- [GPT](https://huggingface.co/docs/transformers/model_doc/openai-gpt)\n",
    "- [GPT-2](https://huggingface.co/transformers/model_doc/gpt2.html)\n",
    "- [Transformer XL](https://huggingface.co/transformers/model_doc/transfo-xl.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa120bca",
   "metadata": {},
   "source": [
    "<img src=\"images/Decoder-Models-1.png\" style=\"width:800px;\" title=\"decoder models\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be159b2f",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Decoder outputs numerical representation for each word in the initial input sequence. Decoder outputs 1-sequence of numbers per input word (a.k.a. feature vectors, or feature tensor). The dimension of the feature vector is defined by the architecture of the model. </span>\n",
    "\n",
    "<span style=\"color:blue\">Where the <b>decoder differs from the encoder is principally with its self-attention mechanism</b>. It's using <b>\"masked self-attention\"</b>. </span>\n",
    "\n",
    "<span style=\"color:red\">For e.g., for the word \"to\" in \"Welcome to NYC\", the vector is unmodified by the word \"NYC\" as all the words on the right (a.k.a. right context words) are masked. Rather than benefitting from all the words on the left and right (i.e., bidirectional context), decoders only have access to the words on single context, either left context words or right context words)</span>. <span style=\"color:green\">The <b>masked self-attention mechanism</b> differs from the self-attention mechanism by <b>using an additional mask to hide the context on either side of the words</b>. The words numerical representation will not be affected by words in the hidden context.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd426658",
   "metadata": {},
   "source": [
    "<img src=\"images/Decoder-Models-2.png\" style=\"width:800px;\" title=\"decoder models\">\n",
    "<br><br>\n",
    "<b>Why should we use a decoder?</b>\n",
    "<img src=\"images/Decoder-Models-3.png\" style=\"width:500px;\" title=\"decoder models\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade9ced7",
   "metadata": {},
   "source": [
    "<b>Casual Language Modelling</b>\n",
    "\n",
    "<span style=\"color:blue\">Ability to generate words or sequence of words given a known sequence of words. This is known as <b>Casual Language Modelling</b> or <b>Natural Language Generation.</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036a9c3a",
   "metadata": {},
   "source": [
    "<b>Example: Guessing the next word in a sentence</b>\n",
    "\n",
    "- Step 1:\n",
    "    - Input to decoder model: \"My\"\n",
    "    - Output of decoder model: Vector/sequence of numbers that represent a single word > Apply a small transformation to the vector so that it maps to all the words known by the model $\\Rightarrow$ Predict the most probably following word. In this case, \"name\" \n",
    "\n",
    "- Step 2: <span style=\"color:green\"><b>Auto-regressive aspect</b> (i.e., we use the past outputs into inputs in the following steps)</span>\n",
    "    - Input to decoder model: \"My name\"\n",
    "    - Output of decoder model: \"is\"\n",
    "    \n",
    "- Step 3: Repeat the word until we are satisfied.\n",
    "    \n",
    "> GPT-2 for example, has a maximum context size of 1024 => We can generate upto 1024 words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1256260a",
   "metadata": {},
   "source": [
    "<img src=\"images/Decoder-Models-4.png\" style=\"width:500px;\" title=\"decoder models\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a97fa03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai_related",
   "language": "python",
   "name": "fastai_related"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
