{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05e74dcb",
   "metadata": {},
   "source": [
    "Source: https://huggingface.co/learn/nlp-course/chapter1/7?fw=pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf88b4a2",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab7daf3",
   "metadata": {},
   "source": [
    "Encoder-decoder models (also called <i>sequence-to-sequence models</i>) <span style=\"color:blue\">use both parts of the Transformer architecture. At each stage, <b>the attention layers of the encoder can access all the words in the initial sentence, whereas the attention layers of the decoder can only access the words positioned before a given word in the input</b>.</span>\n",
    "\n",
    "<span style=\"color:blue\">The <b>pretraining</b> of these models can be done using the <b>objectives of encoder or decoder models</b>, but usually involves something a bit more complex. For instance, [T5](https://huggingface.co/t5-base) is pretrained by replacing random spans of text (that can contain several words) with a single mask special word, and the objective is then to predict the text that this mask word replaces.</span>\n",
    "\n",
    "Sequence-to-sequence models are <span style=\"color:blue\">best suited for tasks revolving around generating new sentences depending on a given input, such as <b>summarization</b>, <b>translation</b>, or <b>generative question answering</b>.\n",
    "\n",
    "Representatives of this family of models include:\n",
    "\n",
    "- [BART](https://huggingface.co/transformers/model_doc/bart.html)\n",
    "- [mBART](https://huggingface.co/transformers/model_doc/mbart.html)\n",
    "- [Marian](https://huggingface.co/transformers/model_doc/marian.html)\n",
    "- [T5](https://huggingface.co/t5-base)\n",
    "- ProphNet\n",
    "- mT5\n",
    "- Pegasus\n",
    "- M2M100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2483f04d",
   "metadata": {},
   "source": [
    "<img src=\"images/Encoder-Decoder-Models-1.png\" style=\"width:500px;\" title=\"encoder-decoder models\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4c0db8",
   "metadata": {},
   "source": [
    "<b>Encoder:</b>\n",
    "<span style=\"color:blue\">Generates a numerical representation (which contains information of the meaning of the sequence) for each input word</span>\n",
    "\n",
    "<b>Decoder:</b>\n",
    "<span style=\"color:blue\">We are passing the outputs of the encoder directly to decoder. Additional to the encoder outputs, we also give the decoder a sequence. When prompting the decoder for an output with no initial sequence, we can give it the value that indicates the \"start of a sequence\". </span>\n",
    "\n",
    "<b>Encoder-Decoder:</b>\n",
    "<span style=\"color:blue\">In summary, the encoder accepts a initial sequence as input, and computes predictions and outputs a numeric representations. It has in a sense, encoded the sequence. Then, it sends that over to the decoder for it to decoding. Since the numeroc representations are already generated by encoder, we can discard the encoder after one run. And, the decoder in turn, using this input alongside its usual sequence input, will take a stab at decoding the sequence. The decoder decodes the sequence and outputs a word. The \"start of sequence word\" indicates to the decoder that it should start decoding the sequence.</span>\n",
    "\n",
    "<img src=\"images/Encoder-Decoder-Models-2.png\" style=\"width:600px;\" title=\"encoder-decoder models\">\n",
    "<img src=\"images/Encoder-Decoder-Models-3.png\" style=\"width:600px;\" title=\"encoder-decoder models\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9343a763",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Now that we have both the encoder numeric representation (feature vector) and an initial generated word, we don't need the encoder anymore. Decoder will act in an <b>auto-regressive manner</b> by using the word it has just outputted as input. This in combination with numerical representations output by the encoder, can now be used to generate the second word. We can continue on and on until the decoder outputs a value that we consider as a stopping value (e.g., \".\")</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477655b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T01:49:31.080194Z",
     "start_time": "2023-07-04T01:49:31.062697Z"
    }
   },
   "source": [
    "<img src=\"images/Encoder-Decoder-Models-4.png\" style=\"width:600px;\" title=\"encoder-decoder models\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb30e682",
   "metadata": {},
   "source": [
    "<b>Example: Translation Language Model (or Transduction)</b>\n",
    "    \n",
    "Step 1: Use encoder to generate numerical representation of the english sentence (e.g., \"Welcome to NYC\")\n",
    "Step 2: Cast the encoded numerical representation to decoder along with use of \"Start of sequence word\" and ask it to output the first word (e.g., \"Bievenue\"). Once it outputs the first word, it will be used as input sequence to the decoder alongside encoder's numerical representation. thereby allowing decoder to predict the second word (e.g., \"Ã \"). Finally, we ask decoder to predict the 3rd word (e.g., \"NYC\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96f2cae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T02:07:03.475749Z",
     "start_time": "2023-07-04T02:07:03.463628Z"
    }
   },
   "source": [
    "<img src=\"images/Encoder-Decoder-Models-5.png\" style=\"width:600px;\" title=\"encoder-decoder models\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeb6b75",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><b>Encoder-Decoder models</b> shine because it has both encoders and decoders</span>, and weigths are not necessarily shared across encoder and decoder. Besides, output length can be independent of input length, as encoders and decoders are separated.\n",
    "- <span style=\"color:blue\"><b>Encoder</b>: Understands the sequence and extracts relevant information, and puts them in a vector dense info</span>\n",
    "- <span style=\"color:blue\"><b>Decoder</b>: Sole purpose is to decode numerical representation outputted by encoder. <b>Can specialise in completely different lanuguage, <i>even different modality, such as images or speech</i></b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f19fa8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T02:05:51.285780Z",
     "start_time": "2023-07-04T02:05:51.273549Z"
    }
   },
   "source": [
    "<b>When should I use sequence-to-sequence-model?</b>\n",
    "<img src=\"images/Encoder-Decoder-Models-6.png\" style=\"width:500px;\" title=\"encoder-decoder models\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8272f8",
   "metadata": {},
   "source": [
    "<b>Example: Translation</b>\n",
    "<img src=\"images/Encoder-Decoder-Models-7.png\" style=\"width:600px;\" title=\"encoder-decoder models\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c88eaea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T02:08:52.285250Z",
     "start_time": "2023-07-04T02:08:52.273318Z"
    }
   },
   "source": [
    "<b>Example: Summarization</b>\n",
    "<img src=\"images/Encoder-Decoder-Models-8.png\" style=\"width:700px;\" title=\"encoder-decoder models\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371fda61",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\">Additionally, <b>we can load an encoder and decoder inside an encoder-decoder model</b>. Therefore, according to the specific task we are targetting, we may choose to use specific encoders and decoders which are proven to work on these tasks</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb5a680",
   "metadata": {},
   "source": [
    "<img src=\"images/Encoder-Decoder-Models-9.png\" style=\"width:700px;\" title=\"encoder-decoder models\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67eb00a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai_related",
   "language": "python",
   "name": "fastai_related"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
