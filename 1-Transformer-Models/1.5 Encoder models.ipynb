{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11dfa387",
   "metadata": {},
   "source": [
    "Source: https://huggingface.co/learn/nlp-course/chapter1/5?fw=pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c12380f",
   "metadata": {},
   "source": [
    "# Encoder Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fcb445",
   "metadata": {},
   "source": [
    "Encoder models use only the encoder of a Transformer model. <span style=\"color:blue\">At each stage, the attention layers can access all the words in the initial sentence</span>. These models are often characterized as having <span style=\"color:blue\"><b>“bi-directional” attention</b></span>, and are often called <span style=\"color:blue\"><i><b>auto-encoding models</b></i></span>.\n",
    "\n",
    "<span style=\"color:blue\">The <b>pretraining</b> of these models usually revolves around somehow <b>corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding</b> or reconstructing the initial sentence.</span>\n",
    "\n",
    "Encoder models are  <span style=\"color:blue\">best suited for tasks requiring an understanding of the full sentence, such as <b>sentence classification, named entity recognition (and more generally word classification), and extractive question answering</b></span>.\n",
    "\n",
    "Representatives of this family of models include:\n",
    "\n",
    "- [ALBERT](https://huggingface.co/transformers/model_doc/albert.html)\n",
    "- [BERT](https://huggingface.co/transformers/model_doc/bert.html)\n",
    "- [DistilBERT](https://huggingface.co/transformers/model_doc/distilbert.html)\n",
    "- [ELECTRA](https://huggingface.co/transformers/model_doc/electra.html)\n",
    "- [RoBERTa](https://huggingface.co/transformers/model_doc/roberta.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1341f70",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><b>The encoder outputs 1-sequence of numbers per input words (a.k.a. feature vector or a feature tensor)</b>. The dimension of the feature vector is defined by the architecture of the model. For the base BEST model, it's 768. These representations contain the value of the words that <b>contextualize</b>. We could say that the vector of 769 values (feature vector) holds the \"meaning\" of the word along with its context in the text. This is done with the help of self-attention mechanism.</span>\n",
    "\n",
    "> <span style=\"color:blue\"><b>The idea is that encoders are very powerful at extracting vectors that carry meaningful information of the sequence</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce08b12",
   "metadata": {},
   "source": [
    "<img src=\"images/Encoder-Models-1.png\" style=\"width:800px;\" title=\"encoder models\">\n",
    "<img src=\"images/Encoder-Models-2.png\" style=\"width:800px;\" title=\"encoder models\">\n",
    "<img src=\"images/Encoder-Models-3.png\" style=\"width:800px;\" title=\"encoder models\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4467f77c",
   "metadata": {},
   "source": [
    "<img src=\"images/Encoder-Models-4.png\" style=\"width:800px;\" title=\"encoder models\">\n",
    "\n",
    "<br><br>\n",
    "<b>Why should we use an encoder?</b>\n",
    "<img src=\"images/Encoder-Models-5.png\" style=\"width:500px;\" title=\"encoder models\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68c1c46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T00:25:23.162811Z",
     "start_time": "2023-07-04T00:25:23.146879Z"
    }
   },
   "source": [
    "<b>Example: Masked Language Modelling (MLM)</b> - Predicting the missing word\n",
    "- Encoders, with bi-directional context, are good at guessing words in the middle of a sequence\n",
    "- This requires both semantic and syntacting understanding\n",
    "\n",
    "<img src=\"images/Encoder-Models-6.png\" style=\"width:800px;\" title=\"encoder models\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423b4593",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T00:27:14.193447Z",
     "start_time": "2023-07-04T00:27:14.181050Z"
    }
   },
   "source": [
    "<b>Example: Sentiment analysis</b>\n",
    "\n",
    "<img src=\"images/Encoder-Models-7.png\" style=\"width:800px;\" title=\"encoder models\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6c0020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai_related",
   "language": "python",
   "name": "fastai_related"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
